\chapter{Labeling Methods}

This chapter discusses the methods used to prepare a set of documents, generate topic models, and the creation of topic labels.

\section{Documents Creation}

Wikipedia provides a huge corpus of human annotated text.  DBpedia \cite{lehmann2015dbpedia} is a knowledge based extracted from Wikipedia.  Using the data dumps from DBpedia made the process of parsing the Wikipedia data much easier.  There is a data dump called ``Long Abstracts'' that holds the first, introductory paragraph of an article.  See appendix \ref{appendix:dbpedia} for examples of the input from DBpedia.  Each long abstract is used as a separate document.

In addition to the long abstracts, DBpedia also provide data dumps for just about every part of Wikipedia.  Of interest here is the category hierarchy.  The data dumps for the category hierarchy first list all the categories, giving the full name of the category.  Second the data dump lists the hierarchical relationship among the categories and the articles that are in each category.  

One complication to this data is that content creators on Wikipedia will also combine or rename articles and leave a redirect in its place.  When creating the category and article hierarchy, many times the exact article is not present in the list of abstracts, but after following a redirect the referenced article can be found.

Using all the articles in wikipedia would result in a very large dataset of millions of documents.  A subset of the articles is prepared by starting with a hand selected category, and following the category hierarchy down a configurable number of steps, including all the articles referenced by those categories.  One other complication with this data, is that the categories may contain cycles.  Care is taken to only include an article once in the subset of data.

\section{Text Preparation}
Sentence splitting and tokenization is preformed using the NLTK: the natural language toolkit \cite{bird2006nltk}.  Sentence splitting is preformed so that when extracting candidate labels from the text, a candidate label is not chosen across sentence boundaries.  Conversely, the LDA topic model calculation just uses a bag of words model so the sentence boundaries are unnecessary for its calculation.

Tokens are prepared by first transliterating non-ASCII characters into their ASCII equivalents. For example changing vowels with diacritics (i.e. resum\'{e} becomes resume.)  In testing it was discovered that the diacritics were applied inconsistently, resulting in topics containing all versions.  Next punctuation is discarded.  This is mainly to correct inconsistently hyphenated words. (i.e. ``long-term'' becomes ``long term'').  Finally, tokens containing non-alphanumeric characters are discarded.  This removes other strange tokens such as urls.  Most of these discarded tokens only appear once, and because LDA is mostly concerned with the co-occurrence of words, discarding these tokens does not change the effectiveness of the LDA calculation.  The original text is kept to prevent candidate labels from spanning across discarded tokens, and to be able to revert candidate labels back to their original form.

Stop words are then removed.  Although LDA does not require the removal of stop words, sometimes it would generate an entire topic made of mostly stop words.  For example a topic was found once including the words ``first'', ``second'', ``third'', etc.  While this topic is interesting from the standpoint of analyzing the algorithm, it is not helpful in getting a general summary of the documents.

\section{Latent Dirichlet Analysis}
The topic models are generated using MALLET \cite{mccallum2002mallet}, a statistical natural language processing package written in Java.  MALLET's LDA tool takes a number of parameters including the number of topic models to generate.  The output of the processing is a mapping of every input word into it's topic.  This output is parsed and saved to the model.  A discussion of the hyper-parameters used is including in Section \ref{section:methods-hyper}.

\section{Candidate Labels}
As mentioned in the introduction, the creation of candidate labels from the text of the documents is one of the goals of this Thesis.  The following subsections describe methods for generating candidate titles:

\subsection{Chunked Noun Phrase}
This is the method Mei et al. \cite{mei2007automatic} used as mentioned in section \ref{section:related-work-labeling}.  First, a list of candidate labels are generated by extracting the chunked noun phrases from the document.  The chunking uses NLTK's part of speech tagging functionality.

\subsection{N-Gram}
A brute-force method of candidate label generation is to simply use all N-grams of words from the input text.  This can generate a very large list of candidate labels.  Depending on the method used to evaluate the candidate labels, this could be prohibitively expensive.  If the evaluation metric is easy to calculate; however, this can ensure that no good labels are missed when using more restrictive methods such as noun phrase chunking.

\section{Ranking Candidate Labels}
The following subsections describe many methods for choosing and ranking candidate labels against the topic models.

\subsection{KL Divergence of Co-occurring Words}
This methods collects all the words appearing close to the candidate label in the text. A label is ranked according to how closely the distribution of the co-occurring words matches a topic model.  Mei et al. \cite{mei2007automatic} used the Kullback-Leibler divergence to determine which distribution matches.

\subsection{Term Frequency}
Looking at the top 10 words for a topic, it is tempting to just combine those words into a meaningful phrase, and use that as the model.  One way to do this is to score the labels just looking at the words present in the label.  Simply count the number of times the words in the candidate label are mapped to a topic.

\subsection{TF-IDF}
A good candidate label will not only describe the topic model, but it should also be distinct from other topics.  One way to do this is to borrow the concept of inverse document frequency (IDF) from information retrieval research and compute the inverse topic frequency.  This evaluation metric, looks at each word individually, and computes the number of times it occurs in the topic, then divides that by the log of the percentage of topics that contain that word.  The score of the label is then the sum of these scores.

\subsection{Cosine Similarity}
One problem with summing term frequencies, is that it can favor long labels.  Cosine similarity is a method of normalizing document length.  This method is to compare the words in the label with the frequency of words in the topic.

\section{Cosine Similarity Performance Optimization}
Giving a score to every candidate label for every topic can be time consuming.  One way to reduce the time required to find the best labels is to use a branch and bound algorithm.  First all the candidate labels are organized into a prefix trie where each node in the tree is a word.  A brute force method would be to visit every node in the trie calculating the score of that node.  Using the branch and bound algorithm can significantly reduce the number of nodes that need to be visited.  For example, if a cosine similarity is used the score of topic $t$ and label $l$, $S_{t,l}$ is:

\begin{equation}
S_{t,l} = \frac{\sum_{i=0}^{n} tf(w_i,t)tf(w_i,l)}{\sqrt{\sum_{i=0}^{n} tf(w_i,t)^2}\sqrt{\sum_{i=0}^{n} tf(w_i,l)^2}}
\end{equation}

where $tf(w_i,t)$ is the term-frequency of word $w_i$ in topic $t$, and $tf(w_i,l)$ is the term-frequency of word $w_i$ in label $l$.  The term in the denominator for the length of the the topic vector can be ignored because it will be the same for all candidate labels, and therefore does not affect the ranking.  To bound the number of nodes visited the maximum score of a candidate label prefix is calculated.  Using the length of the longest candidate label $N$, and the largest term frequency for the topic:  $max_i tf(w_i,t)$, the assumption is made that terms will not be repeated, each additional term will have the largest term frequency, and the candidate label will be of length $N$.

\begin{equation}
\max S_{t,l_p} = \frac{\sum_{i=0}^{n} tf(w_i,t)tf(w_i,l_p) + \max_i tf(w_i,t) * (N-p)}{\sqrt{N}}
\end{equation}

If the trie is visited in descending order according to the topic word frequencies, when a bound condition is reached the rest of the children of the current node can also be skipped.

\section{First Order Preprocessing}
During initial testing, the method that Mei use in \cite{mei2007automatic} produced poor labels when using a phrase's sentences as its context.  The exact methods Mei used were not entirely clear from his paper.  This method was modified to first reduce the number of labels under consideration by choosing the top ten labels that match the topics according to the cosine similarity of the word-topic frequency.  Then Mei's methods were used to choose the best label from that set.

\section{Discussion of Hyper-parameters}
\label{section:methods-hyper}

\begin{description}
\item [Subset Levels] The number of levels deep to select articles and categories from.  This was set using trial an error until a suitably sized dataset was achieved for each of the starting categories.
\item [Dataset Size] The size of the dataset was increased until the dataset could no longer run comfortably in memory.  
\item [Starting Categories] ``Computer Science'', ``Culture'', and ``Popular Music''
\item [Number of Topics] 100, and 30 - Most of the papers used 30 topics.  Using higher and lower numbers of topics helps to compare the results against other topic sizes.
\item [Number of Iterations] 2000 - After about 2000 iterations, there is little benefit to running more.
\item [Use Symmetric Alpha] false - Setting this to false allows there to be un-evenly sized topics.  This generally provides more coherent topics as one would not expect all the actual topics to be of the same size.
\end{description}

\section{Corpus Statistics}
Table \ref{table:corpus-stats} lists some statistics from the Wikipedia dataset that was used to run these experiments.
\begin{table}
	\begin{center}
	\begin{spacing}{2.0}
		\begin{tabular}{lrrr}
			\hline
			& ``Computer Science'' & ``Culture'' & ``Popular Music''\\
			\hline
			Subset Levels & 5 & 2 & 4 \\
			Articles & 51,799 & 37,860 & 78,204 \\
			Categories & 1,427 & 750 & 2,361 \\
			Words & 6,661,293 & 5,586,848 & 8,254,562 \\
			\hline
		\end{tabular}
		\caption{Corpus Statistics}
		\label{table:corpus-stats}
	\end{spacing}
	\end{center}
\end{table}