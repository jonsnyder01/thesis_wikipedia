\chapter{Related Work}
\section{Opinion Mining}
Opinion Mining is the process of collecting and categorizing opinions about a product.  This area of research is closely related to topic models.  Jackson et. al. \cite{jackson2002concept} present methods for manually tagging and clustering user responses.  Their methods involve having 10 people manually cluster survey responses, then combine their clusters using multidimensional scaling \cite{kruskal1978multidimensional} and k-means clustering \cite{Hartigan1979algorithm}.  Finally the researchers decide on the appropriate number of clusters and approve cluster labels.  This method creates good rigor and eliminates some bias by not creating a tagging scheme before hand, but is quite labor intensive.

Product reviews can be an especially good source of information, and supply a large dataset for research.  Lee et al. \cite{lee2008opinion} analyzed reviews on specific products to extract product features and then detect the sentiment of those features. Gamon et al. \cite{gamon2005pulse} describes a system to extract keywords from product reviews and then detect the sentiment around those keywords.  The keywords are then shown in a treemap and color coded according to sentiment.  Zeigler et al. \cite{ziegler2008mining} built a similar system, but applied it to online survey responses.  Although detecting sentiment can provide important information from customer feedback data, a first step is to find good ways of extracting the product features or concerns people are having.  The opinion mining methods here focus more on sentiment analysis and suffer from problems related to synonymy and polysemy.  These papers used term frequency-inverse document frequency \cite{sparck1972statistical} metrics to extract key words and phrases from the text.

Ganisen et al. \cite{ganesan2010opinosis} developed a tool called Opinosis which seeks to create abstractive summaries from sets of sentences.  The domain they tested on was product reviews.  They created a graph representation of all the sentences for a particular topic and then found the most used path in the graph that parses as a complete sentence.

\section{Topic Modeling}
Simply understanding the topics can be a large part of understanding a users concerns in a customer feedback situation.  Indeed, Rilof et al. \cite{riloff2005exploiting} find that topic-filtering and subjectivity filtering are complimentary (i.e. that topics generally contain a cohesive sentiment).  In a study of various opinion mining systems, Pang \cite{pang2008opinion} found that Topic extraction is important for documents containing (1) comparative studies or (2) discussions of various features, aspects, or attributes. 

Lee et al. \cite{lee2010empirical} studied 4 text mining methods including Latent Semantic Analysis (LSA) which analyzes which words occur frequently together to find key phrases and topics; Probabilistic Latent Semantic Analysis (pLSA) which introduces probability theory to determine one topic for every document; Latent Dirichlet Analysis (LDA) which extends the probabilistic model to allow for one document to contain a set of topics with a probability distribution; and Contextual Topic Modeling (CTM) \cite{blei2007correlated} which extends LDA to use a logistic normal distributions instead of dirichlet.

Hong et al. \cite{hong2010empirical} applied LDA to Twitter messages to see if it could predict the topics twitter users would write about in the future.  LDA proved to be useful compared to other methods.  A large part of the research was finding the best way to model documents.  For example, should the document be defined as each Twitter message or an aggregate of all of a user’s messages?  Hong et al. found that larger documents provided better results.

Recent work in the field of LDA research has focused on extending the LDA to model more than just topics, documents, and words.  Mei et al. \cite{mei2007topic} added sentiment to the model so that each topic contains positive, negative, and neutral distributions of words.  Xia  \cite{xia2011feature} extended the LDA model by adding user information.  Chang et al. \cite{chang2009latent} added the idea of a sentence to the LDA model.  

\section{Labeling Topic Models}
\label{section:related-work-labeling}
Mei et al. \cite{mei2007automatic} was one of the first to tackle the problem of labeling topic models.  He extracted a set of candidate labels from a representative corpus.  Then he created a distribution of words for each candidate label using the words that occurred around the candidate labels in the representative corpus.  Using these distributions of words, he matched the labels to the topic models.  Mei evaluated a label using manual surveys of label preferences.  Mei was able to achieve good results.

Indeed Mei's methods have been applied to other domains with success.  Magatti \cite{magatti2009automatic} used Google Directory names as candidate labels with the content of the pages as the representative distribution of words.  Lau et al \cite{lau2011automatic} used Wikipedia article titles as candidate labels with the article’s content as the representative distribution of words.  Xia \cite{wang2010probabilistic} used scientific abstracts as the representative distribution of words, and the abstract keywords as candidate labels.  Ramage \cite{ramage2009labeled} used Del.icio.us web page tags.

Chang et al. \cite{chang2009latent} used topic models to find the most representative sentences for a particular topic model, and use that to provide an extractive summary of the documents.  Although Chang et al. wasn’t specifically trying to create labels for topic models, their approach could be helpful in finding good labels for topic models.  